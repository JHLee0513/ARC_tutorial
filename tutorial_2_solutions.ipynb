{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ConvNets (CNN) in keras\n",
    "\n",
    "Now that you have some idea of how to use keras, we will use that to build a convolutional neural network! It is the main type of network that we will utilize to program our self-driving cars.\n",
    "\n",
    "\n",
    "# The problem/dataset\n",
    "\n",
    "As our toy problem, we will look into CIFAR10, this is one of a good, publicly open dataset that can be used to test your model for general object classification performance. In this dataset contains 60K images in 32x32 resolution. Not only does this have a good amount of data for proper learning to take place, it has multiple classes to practice multi-class classification task.\n",
    "\n",
    "Citation: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import applications\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We will first load our dataset. The process is as follows:\n",
    "\n",
    "## Mac/Linux-based OS\n",
    "1. Download the python version for CIFAR-10 from [here](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "2. Uncompress the .tar.gz file\n",
    "\n",
    "## Windows\n",
    "1. Download the python version for CIFAR-10 from [here](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_data(address):\n",
    "    data = []\n",
    "    for path in glob(address + \"/*\"):\n",
    "        tmp = unpickle(path)\n",
    "        data.append(tmp)\n",
    "    if (len(data)==0):\n",
    "        print(\"Oh no! data is None, which means it failed to load.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"/Users/Hal/ARC_tutorial/CIFAR_10/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xy(input_list, x, y):\n",
    "    for i in range(len(input_list)):\n",
    "        batch = input_list[i]\n",
    "        labels = batch[b'labels']\n",
    "        data = np.array(batch[b'data']) #3072 length vector\n",
    "        #data = np.reshape(data, (-1,32,32,3))\n",
    "        data = np.reshape(data,(10000, 3, 32, 32)).transpose(0,2,3,1)\n",
    "        x[i*len(labels):(i+1)*len(labels),:,:,:] = data\n",
    "        y[i*len(labels):(i+1)*len(labels)] = labels\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = extract_xy(train_data,np.empty((50000,32,32,3)), np.empty(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000,)\n"
     ]
    }
   ],
   "source": [
    "#Here we can check our data shapes to confirm the data has been processed\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "What's good at starting numbers and matrices? Hence we look at some ways to visualize our data before modelling. This is known as part of a process called Explanatory Data Analysis (EDA) where the engineer can look at the data for better understanding of it overall, before diving into crunching numbers.\n",
    "\n",
    "\n",
    "## Look at example Images\n",
    "In the case of Computer Vision, it is helpful for us to actually look at some of the images and some general information about the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choose any ith image\n",
    "idx = np.random.choice(np.arange(50000))\n",
    "\n",
    "#get first image\n",
    "sample = x_train[idx,:,:,:]\n",
    "tmp = sample\n",
    "sample.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x150017a20>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize = (2,2))\n",
    "plt.imshow((sample * 255).astype(np.uint8), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the images do not look to well, since they are small in size (32 by 32). Nonetheless, this can give us physical images to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y(output class) distribution\n",
    "\n",
    "We can also look at the labels to get some ideas about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = np.zeros(10)\n",
    "\n",
    "for label in y_train:\n",
    "    class_count[int(label) - 1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(10),class_count)\n",
    "plt.ylabel(\"image count\")\n",
    "plt.xlabel(\"class label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific data, the providers of data ensured that we are given exactly 5K training image per class, and 1K image per class in the testing dataset. However, in real life scenario, this will come useful in giving yourself general understanding of distribution of data depending on classes. For instance, if a class had significantly more data then others, this may lead to be problem, which should be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer = layers.Input(shape=(32,32,3))\n",
    "    conv1 = Conv2D(32, (3,3), strides=1, activation='relu', padding = 'same') (input_layer)\n",
    "    pool1 = MaxPooling2D((2,2))(conv1)\n",
    "    conv2 = Conv2D(64, (3,3), strides=1, activation='relu', padding = 'same') (pool1)\n",
    "    pool2 = MaxPooling2D((2,2))(conv2)\n",
    "    conv3 = Conv2D(128, (3,3), strides=1, activation='relu', padding = 'same') (pool2)\n",
    "    pool3 = MaxPooling2D((2,2))(conv3)\n",
    "    conv4 = Conv2D(256, (3,3), strides=1, activation='relu', padding = 'same') (pool3)\n",
    "    glob = GlobalMaxPooling2D() (conv4)\n",
    "    fc1 = Dense(512) (glob)\n",
    "    fc1 = Dropout(rate= 0.4) (fc1)\n",
    "    fc2 = Dense(1024) (fc1)\n",
    "    fc2 = Dropout(rate= 0.4)(fc2)\n",
    "    fc3 = Dense(10, activation = 'softmax') (fc2)\n",
    "    model = Model(input_layer, fc3)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (from keras.applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_xception():\n",
    "    model = applications.xception()\n",
    "    model.compile(optimizer = Adam(), loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile, train, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 32, 32, 3) (20000, 32, 32, 3) (30000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Since test data is separetly provided, we do split for train/val\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,test_size=0.4, random_state=42)\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our training data\n",
    "train_data = load_data(\"/Users/Hal/ARC_tutorial/CIFAR_10/test\")\n",
    "x_test, y_test = extract_xy(train_data,np.empty((10000,32,32,3)), np.empty(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale all our data\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_val /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_classes = 10\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_val = to_categorical(y_val, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(type = 'custom'):\n",
    "    if (type=='custom'):\n",
    "        model = build_model()\n",
    "        model.compile(optimizer = Adam(), loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "        model.fit(x_train, y_train, batch_size = 16, epochs = 30, shuffle = True, validation_data=(x_val,y_val),\n",
    "                  verbose = 1)\n",
    "        model.evaluate(x_test, y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_7 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,055,562\n",
      "Trainable params: 1,055,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30000 samples, validate on 20000 samples\n",
      "Epoch 1/30\n",
      " 3168/30000 [==>...........................] - ETA: 3:04 - loss: 2.2263 - acc: 0.1392"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-e06a98e4c5fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-120-fcdf5ca78328>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(type)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         model.fit(x_train, y_train, batch_size = 16, epochs = 30, shuffle = True, validation_data=(x_val,y_val),\n\u001b[0;32m----> 6\u001b[0;31m                   verbose = 1)\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now used various CNN models for the image classification task with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
