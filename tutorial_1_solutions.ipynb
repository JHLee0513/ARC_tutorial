{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to keras\n",
    "\n",
    "This is introductory kernel for ARC members to get used to keras interface. The tutorial will first begin by developing a simple Multi-Layer Perceptron (MLP) model.\n",
    "\n",
    "MLP: https://en.wikipedia.org/wiki/Multilayer_perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "To use packages such as keras and numpy, we have to make sure that we have them installed and import into our code for use. The usual syntax to import packages are:\n",
    "\n",
    "```import xxx``` imports the whole package\n",
    "\n",
    "```import xxx.sss``` import only the subpackage of xxx \n",
    "\n",
    "```from xxx import sss``` equiavelent to above, but does during call requires sss not xxx.sss\n",
    "\n",
    "```import xxx as nnn``` set a nickname for import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We will first start by generating a synthetic data for this tutorial. For the next tutorial, we will be building a CNN based on 'real' data using the MNIST dataset. You are welcome to try different values for the data and experiment. Take the method below for granted, it generates a simple synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n,d,sigma):\n",
    "    x = np.random.randn(d,n)\n",
    "    w = []\n",
    "    for i in range(1,d + 1):\n",
    "        w.append(i / k)\n",
    "        \n",
    "    y = np.matmul(w,x) + sigma * np.random.randn(n)\n",
    "    \n",
    "    return x, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500 #number of samples\n",
    "k = 10 #number of features\n",
    "s = 1 #standard deviation\n",
    "\n",
    "x_train, y_train, w_true = generate_data(n,k,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape # returns tuple showing the shape of matrix 'x_train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "Here we split our data into training and validation set, we won't handle test set in this specific tutorial. As we recall, holding a subset of our data for ensuring our model doesn't overfit is important, as shown below:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*vuZxFMi5fODz2OEcpG-S1g.png\" width=\"400\" height=\"300\">\n",
    "Figure: training/test error tradeoff and Overfitting \n",
    "\n",
    "https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x,y):\n",
    "    n = x.shape[1]\n",
    "    indexes = np.random.permutation(n)\n",
    "    cutoff = int(n * 0.8)\n",
    "    TRAIN = indexes[0: cutoff]\n",
    "    VAL = indexes[cutoff::]\n",
    "\n",
    "    train_data = x[:, TRAIN]\n",
    "    train_label = y[TRAIN]\n",
    "    val = x[:,VAL]\n",
    "    val_label = y[VAL]\n",
    "    return train_data, train_label, val, val_label\n",
    "    \n",
    "train_x, train_y, val_x,val_y = split(x_train,y_train)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.T\n",
    "val_x = val_x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code starts from here!\n",
    "From this point onwards, you will have to complete parts of the code to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds a MLP model\n",
    "def build_model():\n",
    "    K.clear_session()\n",
    "    a = layers.Input(shape=(10,))\n",
    "    b = layers.Dense(32, activation = 'relu') (a)\n",
    "    c = layers.Dense(64, activation = 'relu') (b)\n",
    "    d = layers.Dropout(rate = 0.5) (c)\n",
    "    e = layers.Dense(1, activation = None)(d)\n",
    "    model = Model(inputs=a, outputs=e)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "#summary shows you the entire model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your model finished, you need to compile your model with respective loss function and an optimizer for it to perform gradient descent. Also, we can define metric that will give us good performance value to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr = 0.1), loss = 'mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x,train_y, epochs = 15, batch_size= 100, validation_data=(val_x,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_x, val_y, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats!\n",
    "\n",
    "You just built your own neural network! You have already familiarized yourself with keras. Next time we will build something that pertains more to our actual project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
